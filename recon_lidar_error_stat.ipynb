{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from osgeo import gdal, gdalconst\n",
    "from sklearn import cross_validation\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import normalize\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import linear_model\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from scipy import stats\n",
    "from datetime import datetime, date\n",
    "import os\n",
    "import matplotlib\n",
    "import calendar\n",
    "from ulmo import cdec\n",
    "import pandas as pd\n",
    "import utm\n",
    "import numpy as np\n",
    "from datetime import date, datetime\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.preprocessing import Normalizer, Imputer\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def init():\n",
    "    global site_names, latitudes, longitudes, elevations, all_sites, dates, merced_date_list, tuolumne_date_list\n",
    "    site_names = ['GFL', 'TNY', 'SNF', 'PGM', 'STR']\n",
    "    latitudes = [37.765, 37.838, 37.827, 37.667, 37.637]\n",
    "    longitudes = [-119.773, -119.448, -119.497, -119.625, -119.55]\n",
    "    elevations = [7000., 8150., 8700., 7000., 8200.]\n",
    "    all_sites, dates = get_cdec(site_names, latitudes, longitudes, elevations, '2014-02-01', '2014-05-01')\n",
    "    merced_date_list = [datetime(2014, 4, 6), datetime(2014, 4, 14), datetime(2014, 4, 23), datetime(2014, 4, 29)]\n",
    "    tuolumne_date_list = [datetime(2014, 3, 23), datetime(2014, 4, 7), datetime(2014, 4, 13), datetime(2014, 4, 20),\n",
    "                         datetime(2014, 4, 28)]\n",
    "def resample_lidar(src_fn, dst_fn, pixel_spacing):\n",
    "    # Source\n",
    "    src_filename = src_fn\n",
    "    src = gdal.Open(src_filename, gdalconst.GA_ReadOnly)\n",
    "    src_proj = src.GetProjection()\n",
    "    src_geotrans = src.GetGeoTransform()\n",
    "    x_size = src.RasterXSize\n",
    "    y_size = src.RasterYSize\n",
    "\n",
    "    # We want a section of source that matches this:\n",
    "    match_proj = src_proj\n",
    "    match_geotrans = (src_geotrans[0], pixel_spacing, src_geotrans[2], src_geotrans[3], src_geotrans[4], -pixel_spacing)\n",
    "    new_x_size = int(x_size * src_geotrans[1] / pixel_spacing)\n",
    "    new_y_size = int(y_size * src_geotrans[5] / (-pixel_spacing))\n",
    "    # Output / destination\n",
    "    dst_filename = dst_fn\n",
    "    dst = gdal.GetDriverByName('GTiff').Create(dst_filename, new_x_size, new_y_size, 1, gdalconst.GDT_Float32)\n",
    "    dst.SetGeoTransform( match_geotrans )\n",
    "    dst.SetProjection( match_proj)\n",
    "\n",
    "    # Do the work\n",
    "    gdal.ReprojectImage(src, dst, src_proj, match_proj, gdalconst.GRA_Average)\n",
    "\n",
    "    del dst # Flush\n",
    "\n",
    "def reproject_reconstruction(src_fn, match_fn, dst_fn, lidar=False):\n",
    "    src_filename = src_fn\n",
    "    src = gdal.Open(src_filename, gdalconst.GA_ReadOnly)\n",
    "    src_proj = src.GetProjection()\n",
    "    src_geotrans = src.GetGeoTransform()\n",
    "    \n",
    "    # We want a section of source that matches this:\n",
    "    match_filename = match_fn\n",
    "    match_ds = gdal.Open(match_filename, gdalconst.GA_ReadOnly)\n",
    "    match_proj = match_ds.GetProjection()\n",
    "    match_geotrans = match_ds.GetGeoTransform()\n",
    "    wide = match_ds.RasterXSize\n",
    "    high = match_ds.RasterYSize\n",
    "    \n",
    "    # Output / destination\n",
    "    dst_filename = dst_fn\n",
    "    dst = gdal.GetDriverByName('GTiff').Create(dst_filename, wide, high, 1, gdalconst.GDT_Float32)\n",
    "    dst.SetGeoTransform( match_geotrans )\n",
    "    dst.SetProjection( match_proj)\n",
    "    if not lidar:\n",
    "        gdal.ReprojectImage(src, dst, src_proj, match_proj, gdalconst.GRA_Bilinear)\n",
    "    else:\n",
    "        gdal.ReprojectImage(src, dst, src_proj, match_proj, gdalconst.GRA_Average)\n",
    "    del dst\n",
    "    \n",
    "def add_border(image):\n",
    "    image_shape = image.shape\n",
    "    new_image = np.zeros((image_shape[0] + 2, image_shape[1] + 2))\n",
    "    new_image[1:-1, 1:-1] = image\n",
    "    return new_image\n",
    "\n",
    "def get_cdec(site_names, latitudes, longitudes, elevations, start_time, end_time, bool_dates=True):\n",
    "    all_sites = {}\n",
    "    aspect = gdal.Open(\"3m_data/Merced_500m_ASP.tif\")\n",
    "    aspect_raster = aspect.ReadAsArray()\n",
    "    aspect_georeference = aspect.GetGeoTransform()\n",
    "    if bool_dates:\n",
    "        dates = []\n",
    "    for site, lat, lon, elev in zip(site_names, latitudes, longitudes, elevations):\n",
    "        temp_data = cdec.historical.get_data([site], [3, 18], 'monthly', start_time, end_time)\n",
    "        temp_swe = temp_data[site]['3']['value'].values\n",
    "        temp_sd = temp_data[site]['18']['value'].values\n",
    "        temp_density = temp_swe / temp_sd\n",
    "\n",
    "        new_df = pd.DataFrame({'swe, inch': temp_swe, 'sd, inch': temp_sd, 'density': temp_density},\n",
    "                              index=temp_data[site]['3']['value'].index)\n",
    "        if bool_dates:\n",
    "            dates = temp_data[site]['3']['value'].index\n",
    "        new_dict = {}\n",
    "        new_dict['data'] = new_df\n",
    "        new_dict['lat_lon'] = [lat, lon]\n",
    "        new_dict['elev'] = elev * 0.3048\n",
    "        new_dict['utm'] = utm.from_latlon(lat, lon)\n",
    "        new_dict['aspect'] = aspect_raster[int((new_dict['utm'][0] - aspect_georeference[0]) / aspect_georeference[1]),\n",
    "                                           int((new_dict['utm'][1] - aspect_georeference[3]) / aspect_georeference[5])]\n",
    "        all_sites[site] = new_dict\n",
    "    if bool_dates:\n",
    "        return all_sites, dates\n",
    "    else:\n",
    "        return all_sites\n",
    "    \n",
    "def get_spatial_density(all_sites, dates, day_need):\n",
    "    X = np.empty((0, 3))\n",
    "    y = np.zeros((len(all_sites), 4))\n",
    "\n",
    "    for i, (key, info) in enumerate(all_sites.iteritems()):\n",
    "        utm_location = np.array([np.sqrt(info['utm'][0]**2 + info['utm'][1]**2), info['elev'], info['aspect']])\n",
    "        X = np.vstack((X, utm_location))\n",
    "        for j, temp_date in enumerate(dates):\n",
    "            y[i, j] = info['data']['density'][temp_date]\n",
    "\n",
    "    dem = gdal.Open('3m_data/Merced_500m_DEM.tif')\n",
    "    dem_georeference = dem.GetGeoTransform()\n",
    "    dem_raster = dem.ReadAsArray()\n",
    "    dem_xRasterSize = dem.RasterXSize\n",
    "    dem_yRasterSize = dem.RasterYSize\n",
    "    aspect_raster = gdal.Open('3m_data/Merced_500m_ASP.tif').ReadAsArray()\n",
    "    y_range = np.arange(dem_georeference[3] + 0.5 * dem_georeference[5], \n",
    "                        dem_georeference[3] + (dem_xRasterSize + 0.5) * dem_georeference[5], \n",
    "                        dem_georeference[5])\n",
    "    x_range = np.arange(dem_georeference[0] + 0.5 * dem_georeference[1], \n",
    "                        dem_georeference[0] + (dem_yRasterSize + 0.5) * dem_georeference[1], \n",
    "                        dem_georeference[1])\n",
    "    x_grid, y_grid = np.meshgrid(x_range, y_range)\n",
    "    x_predict = np.column_stack((np.column_stack((np.sqrt((x_grid.flatten()**2 + y_grid.flatten()**2)), \n",
    "                                                  dem_raster.flatten())), \n",
    "                                 aspect_raster.flatten()))\n",
    "    x_predict_not_nan_idx = np.where(x_predict[:, 1] > 100.)\n",
    "    x_predict_new = x_predict[x_predict[:, 1] > 100.]\n",
    "\n",
    "    imp = Imputer()\n",
    "    y = imp.fit_transform(y)\n",
    "\n",
    "    gp = gaussian_process.GaussianProcess(theta0=1e-2, thetaL=1e-4, \n",
    "                                          thetaU=1., nugget=0.01, optimizer='Welch', random_start=100)\n",
    "    gp.fit(X, y[:, 2])\n",
    "    april_y_total = np.zeros(len(x_predict))\n",
    "    april_y_total[x_predict_not_nan_idx] = gp.predict(x_predict_new)\n",
    "    april_y_total = april_y_total.reshape((dem_yRasterSize, dem_xRasterSize))\n",
    "    \n",
    "    gp.fit(X, y[:, 3])\n",
    "    may_y_total = np.zeros(len(x_predict))\n",
    "    may_y_total[x_predict_not_nan_idx] = gp.predict(x_predict_new)\n",
    "    may_y_total = may_y_total.reshape((dem_yRasterSize, dem_xRasterSize))\n",
    "    \n",
    "    date_y_total = april_y_total + (may_y_total - april_y_total) / 30. * day_need\n",
    "    \n",
    "    return date_y_total\n",
    "\n",
    "def error_analysis(all_sites, dates, day_of_april, tree=True, data=False, shift=False, recon_swe_feature=False):\n",
    "    # Calculate snowdensity of the day and get all the spatial data from the folder\n",
    "    sd_fn = \"3m_data/MB201404\" + str(day_of_april).zfill(2) + \"_500m.tif\"\n",
    "    swe_fn = \"3m_data/\" + str(day_of_april).zfill(2) + \"APR2014_Merced.tif\"\n",
    "    \n",
    "    # Check if these file existed, if not, resample and reproject them\n",
    "    if not os.path.isfile(sd_fn):\n",
    "        src_fn = \"3m_data/MB201404\" + str(day_of_april).zfill(2) + \"_SUPERsnow_depth_EXPORT.tif\"\n",
    "        resample_lidar(src_fn, sd_fn, 500.)\n",
    "        \n",
    "    if not os.path.isfile(swe_fn):\n",
    "        src_fn = \"3m_data/\" + str(day_of_april).zfill(2) + \"APR2014.tif\"\n",
    "        reproject_reconstruction(src_fn, sd_fn, swe_fn)\n",
    "    \n",
    "    snow_density = get_spatial_density(all_sites, dates, day_of_april)\n",
    "    \n",
    "    # initialize figure for this function\n",
    "    if not data:\n",
    "        fig, axarr = plt.subplots(ncols=2, nrows=3, figsize=(10, 12))\n",
    "    \n",
    "        im = axarr[0, 0].imshow(snow_density, vmin=0.25, vmax=0.4)\n",
    "        divider = make_axes_locatable(axarr[0, 0])\n",
    "        cax = divider.append_axes(\"right\", size=\"10%\", pad=0.05)\n",
    "        cbar = plt.colorbar(im, cax=cax, ticks=MultipleLocator(0.05), format=\"%.2f\")\n",
    "#     plt.show()\n",
    "\n",
    "    sd_500m = gdal.Open(sd_fn).ReadAsArray()\n",
    "    swe_500m = gdal.Open(swe_fn).ReadAsArray()\n",
    "    sd_swe_500m = sd_500m * snow_density\n",
    "    dem = gdal.Open('3m_data/Merced_500m_DEM.tif').ReadAsArray()\n",
    "    aspect = gdal.Open('3m_data/Merced_500m_ASP.tif').ReadAsArray()\n",
    "    slope = gdal.Open('3m_data/Merced_500m_SLP.tif').ReadAsArray()\n",
    "    northness = gdal.Open('3m_data/Merced_500m_NOR.tif').ReadAsArray()\n",
    "    vegetation = gdal.Open('3m_data/Merced_500m_CHM.tif').ReadAsArray()\n",
    "    if shift:\n",
    "        dem_border = add_border(dem)\n",
    "        dem_shape = dem.shape\n",
    "        dem_space = None\n",
    "        for col_shift in [0, 1, 2]:\n",
    "            for row_shift in [0, 1, 2]:\n",
    "                if dem_space is None:\n",
    "                    temp_dem = dem_border[col_shift:col_shift+dem_shape[0], row_shift:row_shift+dem_shape[1]]\n",
    "                    dem_space = temp_dem.flatten()\n",
    "                else:\n",
    "                    temp_dem = dem_border[col_shift:col_shift+dem_shape[0], row_shift:row_shift+dem_shape[1]]\n",
    "                    dem_space = np.column_stack((dem_space, temp_dem.flatten()))\n",
    "        data_space = np.column_stack((np.column_stack((np.column_stack((np.column_stack((np.column_stack((np.column_stack((dem_space, \n",
    "                                                                                                                           aspect.flatten())), \n",
    "                                                                                                          slope.flatten())),\n",
    "                                                                        northness.flatten())),\n",
    "                                                       vegetation.flatten())),\n",
    "                                      swe_500m.flatten())), \n",
    "                     sd_swe_500m.flatten()))\n",
    "    \n",
    "    else:\n",
    "        data_space = np.column_stack((np.column_stack((np.column_stack((np.column_stack((np.column_stack((np.column_stack((dem.flatten(), \n",
    "                                                                                                                           aspect.flatten())), \n",
    "                                                                                                          slope.flatten())),\n",
    "                                                                        northness.flatten())),\n",
    "                                                       vegetation.flatten())),\n",
    "                                      swe_500m.flatten())), \n",
    "                     sd_swe_500m.flatten()))\n",
    "\n",
    "    # Format the data matrix into expected form\n",
    "    valid_data_space = data_space[data_space[:, -1] > 0]\n",
    "    valid_data_space = valid_data_space[valid_data_space[:, -2] > 0]\n",
    "    lidar_minus_recon = valid_data_space[:, -1] - valid_data_space[:, -2]\n",
    "    valid_data_space = np.column_stack((valid_data_space, lidar_minus_recon))\n",
    "    if shift:\n",
    "        original_outlier_detection_space = valid_data_space[valid_data_space[:, 4] >= 1500., :]\n",
    "        if recon_swe_feature:\n",
    "            feature_col_number = np.arange(0, 14, 1)\n",
    "        else:\n",
    "            feature_col_number = np.arange(0, 13, 1)\n",
    "        feature_col_number = np.append(feature_col_number, [-1])\n",
    "        original_outlier_detection_space = original_outlier_detection_space[:, feature_col_number]\n",
    "        outlier_detection_space = original_outlier_detection_space[:, [4, -1]]\n",
    "    else:\n",
    "        original_outlier_detection_space = valid_data_space[valid_data_space[:, 0] >= 1500., :]\n",
    "        if recon_swe_feature:\n",
    "            original_outlier_detection_space = original_outlier_detection_space[:, [0, 1, 2, 3, 4, 5, -1]]\n",
    "        else:\n",
    "            original_outlier_detection_space = original_outlier_detection_space[:, [0,1,2,3,4,-1]]\n",
    "        outlier_detection_space = original_outlier_detection_space[:, [0, -1]]\n",
    "\n",
    "    # Outlier rejection using robust covariance estimator\n",
    "    classifiers = {\n",
    "        \"robust covariance estimator\": EllipticEnvelope(contamination=.008)}\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(1500, 4000, 500), np.linspace(-7, 1, 500))\n",
    "\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        # fit the data and tag outliers\n",
    "        clf.fit(outlier_detection_space)\n",
    "        y_pred = clf.decision_function(outlier_detection_space).ravel()\n",
    "        threshold = stats.scoreatpercentile(y_pred,\n",
    "                                            100 * 0.008)\n",
    "        y_pred = y_pred > threshold\n",
    "        # plot the levels lines and the points\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        if not data:\n",
    "            subplot = axarr[0, 1]\n",
    "            subplot.set_title(\"Outlier detection\")\n",
    "            subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n",
    "                             cmap=plt.cm.Blues_r)\n",
    "            a = subplot.contour(xx, yy, Z, levels=[threshold],\n",
    "                                linewidths=2, colors='red')\n",
    "            subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n",
    "                             colors='orange')\n",
    "            c = subplot.scatter(outlier_detection_space[:, 0], outlier_detection_space[:, 1], c=y_pred)\n",
    "            subplot.axis('tight')\n",
    "            subplot.legend(\n",
    "                [a.collections[0], c],\n",
    "                ['learned decision function', 'true inliers', 'true outliers'],\n",
    "                prop=matplotlib.font_manager.FontProperties(size=11), loc=3)\n",
    "            subplot.set_xlabel(\"%d. %s\" % (i + 1, clf_name))\n",
    "#     plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n",
    "#     plt.show()\n",
    "\n",
    "    clean_data_space = original_outlier_detection_space[y_pred, :]\n",
    "    \n",
    "    if data:\n",
    "        return clean_data_space\n",
    "\n",
    "    # Calculate the kernel density of the data points in elevation/error 2D space\n",
    "    '''\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.4)\n",
    "    kde.fit(clean_data_space[:, [0, -1]])\n",
    "    point_density = np.exp(kde.score_samples(clean_data_space[:, [0, -1]]))\n",
    "    axarr[1, 0].scatter(clean_data_space[:, 0], clean_data_space[:, -1], c = point_density, edgecolor='none', alpha=0.3)\n",
    "    axarr[1, 0].set_ylim([-1., 1.])\n",
    "    '''\n",
    "#     plt.show()\n",
    "\n",
    "    axarr[1, 0].hist(clean_data_space[:, -1], bins=30)\n",
    "    axarr[1, 0].set_xlim([-0.8, 0.8])\n",
    "#     plt.show()\n",
    "\n",
    "    # Error analysis versus elevation\n",
    "    x_train_elev = clean_data_space[:, 0][:, np.newaxis]\n",
    "    y_train_elev = clean_data_space[:, -1]\n",
    "    \n",
    "    if tree:\n",
    "        dtr = DecisionTreeRegressor(max_depth=4, min_samples_split=10, min_samples_leaf=10)\n",
    "    else:\n",
    "        dtr = linear_model.RANSACRegressor(linear_model.LinearRegression())\n",
    "    dtr.fit(x_train_elev, y_train_elev)\n",
    "    x_predict_elev = np.linspace(1500, 3600, 100)[:, np.newaxis]\n",
    "    y_predict_elev = dtr.predict(x_predict_elev)\n",
    "    axarr[1, 1].scatter(clean_data_space[:, 0], clean_data_space[:, -1], color='k', edgecolor='none')\n",
    "    axarr[1, 1].plot(x_predict_elev, y_predict_elev, '-b', linewidth=5)\n",
    "    axarr[1, 1].set_ylim([-1., 1.])\n",
    "#     plt.show()\n",
    "    \n",
    "    y_elev_residual = y_train_elev - dtr.predict(x_train_elev)\n",
    "    \n",
    "    \n",
    "    # Error analysis of each individual feature vs error\n",
    "    feature_index = [1, 2, 3, 4]\n",
    "    lower_bound = [0., 0., -1., 0.]\n",
    "    upper_bound = [360., 90., 1., 10.]\n",
    "    \n",
    "    \n",
    "    for temp_idx, lb, ub in zip(feature_index, lower_bound, upper_bound):\n",
    "        x_train_feature = clean_data_space[:, temp_idx][:, np.newaxis]\n",
    "        dtr.fit(x_train_feature, y_elev_residual)\n",
    "        x_predict_feature = np.linspace(lb, ub, 100)[:, np.newaxis]\n",
    "        y_predict_feature = dtr.predict(x_predict_feature)\n",
    "        if temp_idx == 3:\n",
    "            axarr[2, 0].scatter(clean_data_space[:, temp_idx], y_elev_residual, color='k', edgecolor='none')\n",
    "            axarr[2, 0].plot(x_predict_feature, y_predict_feature, 'b', linewidth=5)\n",
    "            axarr[2, 0].set_xlim([-0.2, 0.2])\n",
    "            axarr[2, 0].set_ylim([-1., 1.])\n",
    "        if temp_idx == 4:\n",
    "            axarr[2, 1].scatter(clean_data_space[:, 4], y_elev_residual, color='k', edgecolor='none')\n",
    "            axarr[2, 1].plot(x_predict_feature, y_predict_feature, 'b', linewidth=5)\n",
    "            axarr[2, 1].set_xlim([0., 10.])\n",
    "            axarr[2, 1].set_ylim([-0.5, 0.5])\n",
    "    \n",
    "    save_fig_fn = \"figures/MB_APR\" + str(day_of_april).zfill(2) + \".pdf\"\n",
    "    if not tree:\n",
    "        save_fig_fn = \"figures/MB_APR\" + str(day_of_april).zfill(2) + \"_ransac.pdf\"\n",
    "#     plt.savefig(save_fig_fn)\n",
    "    plt.show()\n",
    "    \n",
    "def error_analysis_tb(date_of_season, tree=True, data=False):\n",
    "    # convert date of season to month and day of the month\n",
    "    month = date_of_season.month\n",
    "    day = date_of_season.day\n",
    "    month_name = calendar.month_abbr[month].upper()\n",
    "    # Calculate snowdensity of the day and get all the spatial data from the folder\n",
    "    sd_fn = \"3m_data/TB2014\" + str(month).zfill(2) + str(day).zfill(2) + \"_500m.tif\"\n",
    "    swe_fn = \"3m_data/\" + str(day).zfill(2) + month_name + \"2014_Tuolumne.tif\"\n",
    "    \n",
    "    # Check if these file existed, if not, resample and reproject them\n",
    "    if not os.path.isfile(sd_fn):\n",
    "        src_fn = \"3m_data/TB2014\" + str(month).zfill(2) + str(day).zfill(2) + \"_swe.tif\"\n",
    "        reproject_reconstruction(src_fn, \"3m_data/Tuolumne_500m_DEM.tif\", sd_fn)\n",
    "        \n",
    "    if not os.path.isfile(swe_fn):\n",
    "        src_fn = \"3m_data/\" + str(day).zfill(2) + month_name + \"2014.tif\"\n",
    "        reproject_reconstruction(src_fn, sd_fn, swe_fn)\n",
    "    \n",
    "    # initialize figure for this function\n",
    "    fig, axarr = plt.subplots(ncols=2, nrows=3, figsize=(10, 12))\n",
    "    \n",
    "    sd_500m = gdal.Open(sd_fn).ReadAsArray()\n",
    "    swe_500m = gdal.Open(swe_fn).ReadAsArray()\n",
    "    sd_swe_500m = sd_500m\n",
    "    \n",
    "    im = axarr[0, 0].imshow(sd_swe_500m, vmin = 0, vmax = 1.5)\n",
    "    divider = make_axes_locatable(axarr[0, 0])\n",
    "    cax = divider.append_axes(\"right\", size=\"10%\", pad=0.05)\n",
    "    cbar = plt.colorbar(im, cax=cax, ticks=MultipleLocator(0.3), format=\"%.2f\")\n",
    "    \n",
    "    \n",
    "    dem = gdal.Open('3m_data/Tuolumne_500m_DEM.tif').ReadAsArray()\n",
    "    aspect = gdal.Open('3m_data/Tuolumne_500m_ASP.tif').ReadAsArray()\n",
    "    slope = gdal.Open('3m_data/Tuolumne_500m_SLP.tif').ReadAsArray()\n",
    "    northness = gdal.Open('3m_data/Tuolumne_500m_NOR.tif').ReadAsArray()\n",
    "    data_space = np.column_stack((np.column_stack((np.column_stack((np.column_stack((np.column_stack((dem.flatten(), \n",
    "                                                                                                      aspect.flatten())), \n",
    "                                                                                     slope.flatten())),\n",
    "                                                                    northness.flatten())),\n",
    "                                                   swe_500m.flatten())), \n",
    "                                  sd_swe_500m.flatten()))\n",
    "    print data_space\n",
    "\n",
    "    # Format the data matrix into expected form\n",
    "    valid_data_space = data_space[data_space[:, -1] > 0]\n",
    "    valid_data_space = valid_data_space[valid_data_space[:, -2] > 0]\n",
    "    lidar_minus_recon = valid_data_space[:, -1] - valid_data_space[:, -2]\n",
    "    valid_data_space = np.column_stack((valid_data_space, lidar_minus_recon))\n",
    "    original_outlier_detection_space = valid_data_space[valid_data_space[:, 0] >= 1500., :]\n",
    "    original_outlier_detection_space = original_outlier_detection_space[:, [0, 1, 2, 3, -1]]\n",
    "    outlier_detection_space = original_outlier_detection_space[:, [0, -1]]\n",
    "\n",
    "    # Outlier rejection using robust covariance estimator\n",
    "    classifiers = {\n",
    "        \"robust covariance estimator\": EllipticEnvelope(contamination=.008)}\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(1500, 4000, 500), np.linspace(-7, 1, 500))\n",
    "\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        # fit the data and tag outliers\n",
    "        clf.fit(outlier_detection_space)\n",
    "        y_pred = clf.decision_function(outlier_detection_space).ravel()\n",
    "        threshold = stats.scoreatpercentile(y_pred,\n",
    "                                            100 * 0.008)\n",
    "        y_pred = y_pred > threshold\n",
    "        # plot the levels lines and the points\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        subplot = axarr[0, 1]\n",
    "        subplot.set_title(\"Outlier detection\")\n",
    "        subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n",
    "                         cmap=plt.cm.Blues_r)\n",
    "        a = subplot.contour(xx, yy, Z, levels=[threshold],\n",
    "                            linewidths=2, colors='red')\n",
    "        subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n",
    "                         colors='orange')\n",
    "        c = subplot.scatter(outlier_detection_space[:, 0], outlier_detection_space[:, 1], c=y_pred)\n",
    "        subplot.axis('tight')\n",
    "        subplot.legend(\n",
    "            [a.collections[0], c],\n",
    "            ['learned decision function', 'true inliers', 'true outliers'],\n",
    "            prop=matplotlib.font_manager.FontProperties(size=11), loc=3)\n",
    "        subplot.set_xlabel(\"%d. %s\" % (i + 1, clf_name))\n",
    "#     plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n",
    "#     plt.show()\n",
    "\n",
    "    clean_data_space = original_outlier_detection_space[y_pred, :]\n",
    "    if data:\n",
    "        return clean_data_space\n",
    "    \n",
    "    # Calculate the kernel density of the data points in elevation/error 2D space\n",
    "    '''\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.4)\n",
    "    kde.fit(clean_data_space[:, [0, -1]])\n",
    "    point_density = np.exp(kde.score_samples(clean_data_space[:, [0, -1]]))\n",
    "    axarr[1, 0].scatter(clean_data_space[:, 0], clean_data_space[:, -1], c = point_density, edgecolor='none', alpha=0.3)\n",
    "    axarr[1, 0].set_ylim([-1., 1.])\n",
    "#     plt.show()\n",
    "    '''\n",
    "    axarr[1, 1].hist(clean_data_space[:, -1], bins=30)\n",
    "    axarr[1, 1].set_xlim([-0.8, 0.8])\n",
    "#     plt.show()\n",
    "\n",
    "    # Error analysis versus elevation\n",
    "    x_train_elev = clean_data_space[:, 0][:, np.newaxis]\n",
    "    y_train_elev = clean_data_space[:, -1]\n",
    "    \n",
    "    if tree:\n",
    "        dtr = DecisionTreeRegressor(max_depth=4, min_samples_split=10, min_samples_leaf=10)\n",
    "    else:\n",
    "        dtr = linear_model.RANSACRegressor(linear_model.LinearRegression())\n",
    "    dtr.fit(x_train_elev, y_train_elev)\n",
    "    x_predict_elev = np.linspace(1500, 3600, 100)[:, np.newaxis]\n",
    "    y_predict_elev = dtr.predict(x_predict_elev)\n",
    "    axarr[2, 0].scatter(clean_data_space[:, 0], clean_data_space[:, -1], color='k', edgecolor='none')\n",
    "    axarr[2, 0].plot(x_predict_elev, y_predict_elev, '-b', linewidth=5)\n",
    "    axarr[2, 0].set_ylim([-1., 1.])\n",
    "#     plt.show()\n",
    "    \n",
    "    y_elev_residual = y_train_elev - dtr.predict(x_train_elev)\n",
    "    \n",
    "    \n",
    "    # Error analysis of each individual feature vs error\n",
    "    feature_index = [1, 2, 3]\n",
    "    lower_bound = [0., 0., -1.]\n",
    "    upper_bound = [360., 90., 1.]\n",
    "    \n",
    "    \n",
    "    for temp_idx, lb, ub in zip(feature_index, lower_bound, upper_bound):\n",
    "        x_train_feature = clean_data_space[:, temp_idx][:, np.newaxis]\n",
    "        dtr.fit(x_train_feature, y_elev_residual)\n",
    "        x_predict_feature = np.linspace(lb, ub, 100)[:, np.newaxis]\n",
    "        y_predict_feature = dtr.predict(x_predict_feature)\n",
    "        if temp_idx == 3:\n",
    "            axarr[2, 1].scatter(clean_data_space[:, temp_idx], y_elev_residual, color='k', edgecolor='none')\n",
    "            axarr[2, 1].plot(x_predict_feature, y_predict_feature, 'b', linewidth=5)\n",
    "            axarr[2, 1].set_xlim([-0.2, 0.2])\n",
    "            axarr[2, 1].set_ylim([-1., 1.])\n",
    "    save_fig_fn = \"figures/TB_\" + month_name + str(day).zfill(2) + \".pdf\"\n",
    "    if not tree:\n",
    "        save_fig_fn = \"figures/TB_\" + month_name + str(day).zfill(2) + \"_ransac.pdf\"\n",
    "    plt.savefig(save_fig_fn)\n",
    "    plt.show()\n",
    "    \n",
    "def lidar_recon_anova(site_abbr):\n",
    "    # date_obj\n",
    "    if site_abbr == \"TB\":\n",
    "        date_obj_list = [date(2014, 3, 23), date(2014, 4, 7), date(2014, 4, 13), date(2014, 4, 20), date(2014, 4, 28)]\n",
    "    else:\n",
    "        date_obj_list = [date(2014, 4, 6), date(2014, 4, 14), date(2014, 4, 23), date(2014, 4, 29)]\n",
    "    \n",
    "    # Formatting the entire feature space\n",
    "    error_array_list = []\n",
    "    all_sites, dates = get_cdec(site_names, latitudes, longitudes, elevations, '2014-02-01', '2014-05-01')\n",
    "    for date_obj in date_obj_list:\n",
    "        if site_abbr == \"TB\":\n",
    "            feature_space = error_analysis_tb(date_obj, data=True)\n",
    "        else:\n",
    "            site_names = ['GFL', 'TNY', 'SNF', 'PGM', 'STR']\n",
    "            latitudes = [37.765, 37.838, 37.827, 37.667, 37.637]\n",
    "            longitudes = [-119.773, -119.448, -119.497, -119.625, -119.55]\n",
    "            elevations = [7000., 8150., 8700., 7000., 8200.]\n",
    "            feature_space = error_analysis(all_sites, dates, date_obj.day, data=True)\n",
    "        error_array_list.append(feature_space[:, -1])\n",
    "    f_val, p_val = stats.f_oneway(*error_array_list)\n",
    "    print \"The p-value of one-way ANOVA is\", p_val\n",
    "    \n",
    "def getData(site_abbr, date_time, cv=False, kfold=False, shift=False, with_swe=False):\n",
    "    if cv and kfold:\n",
    "        print \"Cross validation and k-fold cannot be true at the same time\"\n",
    "        return 0\n",
    "    if site_abbr == \"MB\":\n",
    "        day_of_april = date_time.day\n",
    "        data_table = error_analysis(all_sites, dates, day_of_april, data=True, shift=shift, recon_swe_feature=with_swe)\n",
    "        feature = data_table[:, 0:-1]\n",
    "        predict_y = data_table[:, -1]\n",
    "        if cv:\n",
    "            X_train, X_test, y_train, y_test = cross_validation.train_test_split(feature, predict_y, test_size=0.4,\n",
    "                                                                                 random_state=0)\n",
    "            return X_train, X_test, y_train, y_test\n",
    "        elif kfold:\n",
    "            KFold_obj = KFold(n=len(feature), n_folds=5, shuffle=True, random_state=None)\n",
    "            return feature, predict_y, KFold_obj\n",
    "        else:\n",
    "            return feature, predict_y\n",
    "        \n",
    "        \n",
    "# For a particular day of the lidar observation, check the kernel density estimation of the error. Then fit the best \n",
    "# fitted random forest regression model to the data and check how does the error density changes with regard the \n",
    "# initial error\n",
    "def randomForestPredict(site_abbr, date_time, shift=False, kFold=False, cv=True, with_swe=False):\n",
    "    \n",
    "    n_est_list = np.arange(10, 100, 10)\n",
    "    n_min_samples_leaf = np.arange(10, 50, 10)\n",
    "    error = np.zeros((len(n_est_list), len(n_min_samples_leaf)))\n",
    "    if kFold:\n",
    "        feature, predict_y, kf = getData(site_abbr, date_time, shift=shift, kfold=True, with_swe=with_swe)\n",
    "        print feature.shape\n",
    "        for i, n_est in enumerate(n_est_list):\n",
    "            print i\n",
    "            for j, n_sl in enumerate(n_min_samples_leaf):\n",
    "                rf = RandomForestRegressor(n_estimators=n_est, max_features='auto', min_samples_leaf=n_sl, n_jobs=18)\n",
    "                y_predict_list = np.empty(1)\n",
    "                y_test_list = np.empty(1)\n",
    "                for train_index, test_index in kf:\n",
    "                    [X_train, X_test, y_train, y_test] = [feature[train_index], feature[test_index], \n",
    "                                                          predict_y[train_index], predict_y[test_index]]\n",
    "                    rf.fit(X_train, y_train)\n",
    "                    k_fold_predict = rf.predict(X_test)\n",
    "                    y_predict_list = np.append(y_predict_list, k_fold_predict)\n",
    "                    y_test_list = np.append(y_test_list, y_test)\n",
    "                rmse = np.sqrt(mean_squared_error(y_test_list, y_predict_list))\n",
    "                error[i, j] = rmse\n",
    "    elif cv:\n",
    "        X_train, X_test, y_train, y_test = getData(site_abbr, date_time, shift=shift, kfold=False, cv=True, with_swe=with_swe)\n",
    "        for i,n_est in enumerate(n_est_list):\n",
    "            print i\n",
    "            for j,n_sl in enumerate(n_min_samples_leaf):\n",
    "                rf = RandomForestRegressor(n_estimators=n_est, max_features='auto', min_samples_leaf=n_sl, \n",
    "                                           n_jobs=18)\n",
    "                rf.fit(X_train, y_train)\n",
    "                y_predict = rf.predict(X_test)\n",
    "                mse = mean_squared_error(y_test, y_predict)\n",
    "                error[i, j] = np.sqrt(mse)\n",
    "    min_error_idx = np.where(error==np.min(error))\n",
    "    n_est_opt = n_est_list[min_error_idx[0]]\n",
    "    n_sl_opt = n_min_samples_leaf[min_error_idx[1]]\n",
    "    feature, predict_y = getData(site_abbr, date_time, shift=shift, with_swe=with_swe)\n",
    "    final_model = RandomForestRegressor(n_estimators=n_est_opt, max_features='auto', min_samples_leaf=n_sl_opt, n_jobs=18)\n",
    "    final_model.fit(feature, predict_y)\n",
    "    if shift:\n",
    "        final_model_fn = \"regression_models/\" + site_abbr + \"_\" + date_time.strftime(\"%y_%m_%d\") + \"_shift_\"\n",
    "    else:\n",
    "        final_model_fn = \"regression_models/\" + site_abbr + \"_\" + date_time.strftime(\"%y_%m_%d\") + \"_\"\n",
    "    if with_swe:\n",
    "            final_model_fn += \"swe_rf.p\"\n",
    "    else:\n",
    "        final_model_fn += \"rf.p\"\n",
    "    joblib.dump(final_model, final_model_fn)\n",
    "    return error, n_est_opt, n_sl_opt\n",
    "\n",
    "\n",
    "def tune_rf_all_dates(with_swe=False):\n",
    "    date_time_list = [datetime(2014, 4, 6), datetime(2014, 4, 14), datetime(2014, 4, 23), datetime(2014, 4, 29)]\n",
    "    for temp_date_time in date_time_list:\n",
    "        rmse, n_est_opt, n_sl_opt = randomForestPredict(\"MB\", temp_date_time, shift=False, kFold=True, with_swe=with_swe)\n",
    "        plt.imshow(rmse, vmin = np.min(rmse), vmax = np.max(rmse), interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "\n",
    "    # This one is with 9 grids of dem features\n",
    "    for temp_date_time in date_time_list:\n",
    "        rmse, n_est_opt, n_sl_opt = randomForestPredict(\"MB\", temp_date_time, shift=True, kFold=True, with_swe=with_swe)\n",
    "        plt.imshow(rmse, vmin = np.min(rmse), vmax = np.max(rmse), interpolation='nearest')\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "def calculate_1d_kernel_density(X, linspace):\n",
    "    kde = KernelDensity(kernel='gaussian', bandwidth=0.1).fit(X[:, np.newaxis])\n",
    "    density = kde.score_samples(linspace)\n",
    "    return np.exp(density)\n",
    "\n",
    "\n",
    "def plot_err_dist_before_and_after(site_abbr, date_time, shift=False, with_swe=False):\n",
    "    if site_abbr == \"MB\":\n",
    "        feature, y = getData(site_abbr, date_time, shift=shift, with_swe=with_swe)\n",
    "        print feature.shape\n",
    "        y_linspace = np.linspace(-0.5, 0.5, 200)[:, np.newaxis]\n",
    "        y_density = calculate_1d_kernel_density(y, y_linspace)\n",
    "        plt.plot(y_linspace, y_density)\n",
    "        if shift:\n",
    "            rf_model_fn = \"regression_models/\" + site_abbr + \"_\" + date_time.strftime(\"%y_%m_%d\") + \"_shift_\"\n",
    "        else:\n",
    "            rf_model_fn = \"regression_models/\" + site_abbr + \"_\" + date_time.strftime(\"%y_%m_%d\") + \"_\"\n",
    "        if with_swe:\n",
    "            rf_model_fn += \"swe_rf.p\"\n",
    "        else:\n",
    "            rf_model_fn += \"rf.p\"\n",
    "        print rf_model_fn\n",
    "        rf_model = joblib.load(rf_model_fn)\n",
    "        y_predict = rf_model.predict(feature)\n",
    "        y_diff = y - y_predict\n",
    "        y_predict_density = calculate_1d_kernel_density(y_diff, y_linspace)\n",
    "        plt.plot(y_linspace, y_predict_density)\n",
    "        plt.legend([\"Original\", \"Corrected\"])\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "def plot_err_dist_all_dates(with_swe=False):\n",
    "    date_time_list = [datetime(2014, 4, 6), datetime(2014, 4, 14), datetime(2014, 4, 23), datetime(2014, 4, 29)]\n",
    "    for date_time in date_time_list:\n",
    "        plot_err_dist_before_and_after(\"MB\", date_time, shift=True, with_swe=with_swe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_boosting(date_time, shift=False, kFold=False, cv=True, with_swe=False):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_feature_space(all_sites, dates, day_of_april, tree=True, data=False, shift=False, recon_swe_feature=False):\n",
    "    # Calculate snowdensity of the day and get all the spatial data from the folder\n",
    "    sd_fn = \"3m_data/MB201404\" + str(day_of_april).zfill(2) + \"_500m.tif\"\n",
    "    swe_fn = \"3m_data/\" + str(day_of_april).zfill(2) + \"APR2014_Merced.tif\"\n",
    "    \n",
    "    # Check if these file existed, if not, resample and reproject them\n",
    "    if not os.path.isfile(sd_fn):\n",
    "        src_fn = \"3m_data/MB201404\" + str(day_of_april).zfill(2) + \"_SUPERsnow_depth_EXPORT.tif\"\n",
    "        resample_lidar(src_fn, sd_fn, 500.)\n",
    "        \n",
    "    if not os.path.isfile(swe_fn):\n",
    "        src_fn = \"3m_data/\" + str(day_of_april).zfill(2) + \"APR2014.tif\"\n",
    "        reproject_reconstruction(src_fn, sd_fn, swe_fn)\n",
    "    \n",
    "    snow_density = get_spatial_density(all_sites, dates, day_of_april)\n",
    "    \n",
    "    # initialize figure for this function\n",
    "    if not data:\n",
    "        fig, axarr = plt.subplots(ncols=2, nrows=3, figsize=(10, 12))\n",
    "    \n",
    "        im = axarr[0, 0].imshow(snow_density, vmin=0.25, vmax=0.4)\n",
    "        divider = make_axes_locatable(axarr[0, 0])\n",
    "        cax = divider.append_axes(\"right\", size=\"10%\", pad=0.05)\n",
    "        cbar = plt.colorbar(im, cax=cax, ticks=MultipleLocator(0.05), format=\"%.2f\")\n",
    "#     plt.show()\n",
    "\n",
    "    sd_500m = gdal.Open(sd_fn).ReadAsArray()\n",
    "    swe_500m = gdal.Open(swe_fn).ReadAsArray()\n",
    "    sd_swe_500m = sd_500m * snow_density\n",
    "    dem = gdal.Open('3m_data/Merced_500m_DEM.tif').ReadAsArray()\n",
    "    aspect = gdal.Open('3m_data/Merced_500m_ASP.tif').ReadAsArray()\n",
    "    slope = gdal.Open('3m_data/Merced_500m_SLP.tif').ReadAsArray()\n",
    "    northness = gdal.Open('3m_data/Merced_500m_NOR.tif').ReadAsArray()\n",
    "    vegetation = gdal.Open('3m_data/Merced_500m_CHM.tif').ReadAsArray()\n",
    "    if shift:\n",
    "        dem_border = add_border(dem)\n",
    "        dem_shape = dem.shape\n",
    "        dem_space = None\n",
    "        for col_shift in [0, 1, 2]:\n",
    "            for row_shift in [0, 1, 2]:\n",
    "                if dem_space is None:\n",
    "                    temp_dem = dem_border[col_shift:col_shift+dem_shape[0], row_shift:row_shift+dem_shape[1]]\n",
    "                    dem_space = temp_dem.flatten()\n",
    "                else:\n",
    "                    temp_dem = dem_border[col_shift:col_shift+dem_shape[0], row_shift:row_shift+dem_shape[1]]\n",
    "                    dem_space = np.column_stack((dem_space, temp_dem.flatten()))\n",
    "        data_space = np.column_stack((np.column_stack((np.column_stack((np.column_stack((np.column_stack((np.column_stack((dem_space, \n",
    "                                                                                                                           aspect.flatten())), \n",
    "                                                                                                          slope.flatten())),\n",
    "                                                                        northness.flatten())),\n",
    "                                                       vegetation.flatten())),\n",
    "                                      swe_500m.flatten())), \n",
    "                     sd_swe_500m.flatten()))\n",
    "    \n",
    "    else:\n",
    "        data_space = np.column_stack((np.column_stack((np.column_stack((np.column_stack((np.column_stack((np.column_stack((dem.flatten(), \n",
    "                                                                                                                           aspect.flatten())), \n",
    "                                                                                                          slope.flatten())),\n",
    "                                                                        northness.flatten())),\n",
    "                                                       vegetation.flatten())),\n",
    "                                      swe_500m.flatten())), \n",
    "                     sd_swe_500m.flatten()))\n",
    "\n",
    "    # Format the data matrix into expected form\n",
    "    valid_data_space = data_space[data_space[:, -1] > 0]\n",
    "    valid_data_space = valid_data_space[valid_data_space[:, -2] > 0]\n",
    "    lidar_minus_recon = valid_data_space[:, -1] - valid_data_space[:, -2]\n",
    "    valid_data_space = np.column_stack((valid_data_space, lidar_minus_recon))\n",
    "    if shift:\n",
    "        original_outlier_detection_space = valid_data_space[valid_data_space[:, 4] >= 1500., :]\n",
    "        if recon_swe_feature:\n",
    "            feature_col_number = np.arange(0, 14, 1)\n",
    "        else:\n",
    "            feature_col_number = np.arange(0, 13, 1)\n",
    "        feature_col_number = np.append(feature_col_number, [-1])\n",
    "        original_outlier_detection_space = original_outlier_detection_space[:, feature_col_number]\n",
    "        outlier_detection_space = original_outlier_detection_space[:, [4, -1]]\n",
    "    else:\n",
    "        original_outlier_detection_space = valid_data_space[valid_data_space[:, 0] >= 1500., :]\n",
    "        if recon_swe_feature:\n",
    "            original_outlier_detection_space = original_outlier_detection_space[:, [0, 1, 2, 3, 4, 5, -1]]\n",
    "        else:\n",
    "            original_outlier_detection_space = original_outlier_detection_space[:, [0,1,2,3,4,-1]]\n",
    "        outlier_detection_space = original_outlier_detection_space[:, [0, -1]]\n",
    "\n",
    "    # Outlier rejection using robust covariance estimator\n",
    "    classifiers = {\n",
    "        \"robust covariance estimator\": EllipticEnvelope(contamination=.008)}\n",
    "\n",
    "    xx, yy = np.meshgrid(np.linspace(1500, 4000, 500), np.linspace(-7, 1, 500))\n",
    "\n",
    "#     plt.figure(figsize=(10, 5))\n",
    "    for i, (clf_name, clf) in enumerate(classifiers.items()):\n",
    "        # fit the data and tag outliers\n",
    "        clf.fit(outlier_detection_space)\n",
    "        y_pred = clf.decision_function(outlier_detection_space).ravel()\n",
    "        threshold = stats.scoreatpercentile(y_pred,\n",
    "                                            100 * 0.008)\n",
    "        y_pred = y_pred > threshold\n",
    "        # plot the levels lines and the points\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "        Z = Z.reshape(xx.shape)\n",
    "        if not data:\n",
    "            subplot = axarr[0, 1]\n",
    "            subplot.set_title(\"Outlier detection\")\n",
    "            subplot.contourf(xx, yy, Z, levels=np.linspace(Z.min(), threshold, 7),\n",
    "                             cmap=plt.cm.Blues_r)\n",
    "            a = subplot.contour(xx, yy, Z, levels=[threshold],\n",
    "                                linewidths=2, colors='red')\n",
    "            subplot.contourf(xx, yy, Z, levels=[threshold, Z.max()],\n",
    "                             colors='orange')\n",
    "            c = subplot.scatter(outlier_detection_space[:, 0], outlier_detection_space[:, 1], c=y_pred)\n",
    "            subplot.axis('tight')\n",
    "            subplot.legend(\n",
    "                [a.collections[0], c],\n",
    "                ['learned decision function', 'true inliers', 'true outliers'],\n",
    "                prop=matplotlib.font_manager.FontProperties(size=11), loc=3)\n",
    "            subplot.set_xlabel(\"%d. %s\" % (i + 1, clf_name))\n",
    "#     plt.subplots_adjust(0.04, 0.1, 0.96, 0.94, 0.1, 0.26)\n",
    "#     plt.show()\n",
    "\n",
    "    clean_data_space = original_outlier_detection_space[y_pred, :]\n",
    "    \n",
    "    return clean_data_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "\n",
    "#     date_list = [datetime(2014, 3, 23), datetime(2014, 4, 7), datetime(2014, 4, 13), datetime(2014, 4, 20), datetime(2014, 4, 28)]\n",
    "#     for temp_date in date_list:\n",
    "#         error_analysis_tb(temp_date, tree=False)\n",
    "\n",
    "    for day_num in [6, 14, 23, 29]:\n",
    "    # for day_num in [6]:\n",
    "        error_analysis(all_sites, dates, day_num, tree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rf_correct_tuning_result():\n",
    "    init()\n",
    "    tune_rf_all_dates(with_swe=True)\n",
    "    plot_err_dist_all_dates()\n",
    "    plot_err_dist_all_dates(with_swe=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
